{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Falconwatch/llm_course/blob/main/HW1/%D0%94%D0%971.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oY3HD3eyIyC7",
      "metadata": {
        "id": "oY3HD3eyIyC7"
      },
      "source": [
        "# Описание ДЗ1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRly-JkgJOo_",
      "metadata": {
        "id": "rRly-JkgJOo_"
      },
      "source": [
        "На основе семинара 1 предложите 2 метода улучшения построения эмбеддингов вопросов на основе word vectors.\n",
        "\n",
        "За задание можно получить максимум 10 баллов.\n",
        "\n",
        "За каждый метод можно получить максимум 5 баллов.\n",
        "Разбалловка:\n",
        "*   Воспроизводимость и читабельность кода - 1 балл.\n",
        "*   Корректность метода - 1 балл.\n",
        "*   Описание метода в техническом отчете - 2 балла.\n",
        "*   Иновационность - 1 балл.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Информация о сабмите"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Щербаков Игорь Андреевич"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Технический отчет"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3",
      "metadata": {
        "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3"
      },
      "source": [
        "***Введите сюда** подробное описание предложенных методов и экспериментов, с помощью которых вы пришли именно к выбору этих методов. НЕ вставляйте код в эту часть. Описание должно состоять минимум из 2-4 абзацев и содержать следующее: тип модели, параметры, как вы выбрали параметры, какие дальнейшие модификации готовых решений и т.д. вы использовали. Сюда можно включить, например, некоторые хитрости вашей предварительной обработки, описание моделей и мотивацию их использования, описание деталей процесса обучения. Если нужно, вставьте сюда графики, математические формулы.*\n",
        "\n",
        "*Балл за «инновационность» будет присваиваться на основе содержания этой части. Если ваше отличие от бейзлайна это просто почистить тексты от стоп-слов или поменять одну базовую модель для построения word embeddings на другую, этот балл будет 0. Пробуйте разные подходы, модели, экспериментируйте с предварительной обработкой, параметрами и т. д. Можно переопределить уже существующий подход. Это нормально, что некоторые из ваших экспериментов не сработали так, как вы ожидали. Покажите нам, что вы проявили творческий подход и провели несколько экспериментов.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837fbc4d",
      "metadata": {
        "id": "837fbc4d"
      },
      "source": [
        "<h2>Возможные шаги</h2>\n",
        "Какие шаги были бы возможны:\n",
        "<ul> 1. Предобработка данных х</ul>\n",
        "<ul> 2. Подбор модели</ul>\n",
        "<ul> 3. Подбор параметров модели</ul>\n",
        "<ul> 4. Эксперименты с эмбедингами слов (добавление доп размерностей с новой информацией)</ul>\n",
        "<ul> 5. Изменеиение способа получения представления фразы на основе представлений слов</ul>\n",
        "<ul> 6. Модификация алгоритмов поиска похожих: повышение качества и скорости</ul>\n",
        "\n",
        "\n",
        "Подбор разных моделей и их параметров, кажется весьма обыденным, поэтому в этом ноутбуке рассмотрим следующщие подходы: предобработку данных, дополнительные размерности в эмбединг слова, представление фразы на основе слов.\n",
        "\n",
        "<h2>Что попробовал</h2>\n",
        "<h3>Предобработка</h3>\n",
        "В качестве дополнитльной предобработки были испоьзованы:\n",
        "<li> Стемминг алгоритмом Snowball - ок, полезно убирать лишние формы слов, часто опечатки в конце\n",
        "<li> Удаление стоп слов - отказался от него в пользу взвешивания\n",
        "<li> Формировать вектор предложения как взвешенную сумму векторов слов. В качестве весов - значения idf. Идейно подход заменяет выкидывание стоп-слов, занижая вес сильно распространённых слов\n",
        "\n",
        "<h3>Эксперименты с моделью</h3>\n",
        "<li>1\n",
        "<li>2\n",
        "\n",
        "\n",
        "\n",
        "<h3>Поиск похожих</h3>\n",
        "Алгоритм поиска похожих топиков на основе косинусного расстояния был заменнён на реализацию в FAISS, что позволило многократно ускорить поиск и повысить его релевантность.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. *Code*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1",
      "metadata": {
        "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1"
      },
      "source": [
        "*Введите сюда весь код, использованный для получения результатов. Добавьте несколько комментариев и подразделов для навигации по вашему решению.*\n",
        "\n",
        "*В этой части вам предстоит самостоятельно разработать решение задачи и предоставить воспроизводимый код:*\n",
        "- *Использование Python 3;*\n",
        "- *Содержит код для установки всех зависимостей;*\n",
        "- *Содержит код для загрузки всех используемых наборов данных*;\n",
        "- *Содержит код для воспроизведения ваших результатов (другими словами, если проверяющий загрузит ваш блокнот, он сможет выполнить код по ячейкам и получить результаты эксперимента, как описано в разделе методологии)*.\n",
        "\n",
        "\n",
        "*В результате ваш код будет оценен по следующим критериям:*\n",
        "- ***Читаемость**: ваш код должен быть хорошо структурирован, желательно с указанием частей вашего подхода (предварительная обработка, обучение модели, тестирование модели и т. д.).*\n",
        "- ***Воспроизводимость**: ваш код должен воспроизводиться без ошибок в режиме «Выполнить все» (получение экспериментальной части).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.1 Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82a7f821",
      "metadata": {
        "id": "82a7f821",
        "outputId": "34cc9af3-0a83-4ae4-d51b-4802b836cc15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "#Установка бибилотек в окружение\n",
        "#!pip install -q -r requirements.txt\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9RQzpKJkLczr",
      "metadata": {
        "id": "9RQzpKJkLczr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import gensim.downloader as api\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.2 Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdce2db4",
      "metadata": {
        "id": "cdce2db4",
        "outputId": "d769a8f9-0f1f-45f3-d1eb-8029be022e88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-05 17:26:42--  https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [text/plain]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M   177MB/s    in 0.2s    \n",
            "\n",
            "2024-06-05 17:26:43 (177 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "\n",
        "#!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "#!wget https://yadi.sk/i/BPQrUu1NaTduEw -O ./quora.txt\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt -O ./quora.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9e25a5",
      "metadata": {
        "id": "6a9e25a5"
      },
      "outputs": [],
      "source": [
        "data = np.array(list(open(\"./quora.txt\", encoding=\"utf-8\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "oAORpF10drNu"
      },
      "id": "oAORpF10drNu"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "TOKENIZER = WordPunctTokenizer()\n",
        "STEMMER = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "LTfN7SCiduU8"
      },
      "id": "LTfN7SCiduU8",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def preprocess_phrase(phrase):\n",
        "  lower_phrase = phrase.lower()\n",
        "  lower_phrase = lower_phrase.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  tokens = TOKENIZER.tokenize(lower_phrase)\n",
        "  stemmed_tokens = [STEMMER.stem(t) for t in tokens]\n",
        "\n",
        "  return stemmed_tokens\n",
        "\n",
        "preprocess_phrase(\"Hi there!\")"
      ],
      "metadata": {
        "id": "MA2H4OYdd1d8",
        "outputId": "22dd7d55-cd16-40c0-d3ea-c80e5fd948d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MA2H4OYdd1d8",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'there']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = [preprocess_phrase(d) for d in data[:]]"
      ],
      "metadata": {
        "id": "2WTfeLCheqxb"
      },
      "id": "2WTfeLCheqxb",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Формируем глобальный словарь частот\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "data_for_tfidf = [\" \".join(ws) for ws in preprocessed_data]\n",
        "vectorizer.fit_transform(data_for_tfidf)\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "idfs = vectorizer.idf_\n",
        "idfs_normalised = idfs/np.max(idfs)\n",
        "\n",
        "WORDS_IDFS = defaultdict(lambda: 1.0, {w:i for w,i in zip(words, idfs_normalised)})\n",
        "\n",
        "print(WORDS_IDFS[\"what\"])\n",
        "print(WORDS_IDFS[\"is\"])\n",
        "print(WORDS_IDFS[\"dog\"])\n",
        "print(WORDS_IDFS[\"?\"])\n"
      ],
      "metadata": {
        "id": "uV_HVOVMfDtn",
        "outputId": "bdf346c9-fd62-4e3f-ff71-0d4366ad9c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uV_HVOVMfDtn",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14412748635454461\n",
            "0.1572065412341149\n",
            "0.48880240698138905\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_phrase_and_get_idf(phrase):\n",
        "  preprocessed_phrase = preprocess_phrase(phrase)\n",
        "  phrase_idfs = [WORDS_IDFS[word] for word in preprocessed_phrase]\n",
        "  phrase_idfs = phrase_idfs/np.sum(phrase_idfs)\n",
        "  return preprocessed_phrase, np.array(phrase_idfs)\n",
        "\n",
        "process_phrase_and_get_idf(\"what is dog?\")\n"
      ],
      "metadata": {
        "id": "KfqXH32JiH65",
        "outputId": "371b7cbe-54d4-4027-a73d-9f6fc2a4a36c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KfqXH32JiH65",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['what', 'is', 'dog'], array([0.18240835, 0.19896126, 0.61863039]))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.3. Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "dec65766",
      "metadata": {
        "id": "dec65766"
      },
      "outputs": [],
      "source": [
        "MODEL = api.load('glove-twitter-100')\n",
        "WORDS_IN_DICT = MODEL.key_to_index.keys()\n",
        "\n",
        "#model = FastText.load_fasttext_format('wiki.simple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "-ySx_n28KGAX",
      "metadata": {
        "id": "-ySx_n28KGAX"
      },
      "outputs": [],
      "source": [
        "def get_phrase_embedding(phrase):\n",
        "    \"\"\"\n",
        "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
        "    \"\"\"\n",
        "    # 1. lowercase phrase\n",
        "    # 2. tokenize phrase\n",
        "    # 3. average word vectors for all words in tokenized phrase\n",
        "    # skip words that are not in model's vocabulary\n",
        "    # if all words are missing from vocabulary, return zeros\n",
        "\n",
        "\n",
        "    empty_vector = np.zeros([MODEL.vector_size], dtype='float32')\n",
        "    word_vectors = []\n",
        "\n",
        "    phrase_words, words_weights = process_phrase_and_get_idf(phrase)\n",
        "\n",
        "    for word in phrase_words:\n",
        "      if word in WORDS_IN_DICT:\n",
        "        word_vectors.append(MODEL.get_vector(word))\n",
        "      else:\n",
        "        word_vectors.append(empty_vector)\n",
        "    word_vectors = np.array(word_vectors)\n",
        "\n",
        "    phrase_vector = np.dot(words_weights, word_vectors)\n",
        "    return phrase_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "FEpFpJsfUdA4",
      "metadata": {
        "id": "FEpFpJsfUdA4"
      },
      "outputs": [],
      "source": [
        "# compute vector embedding for all lines in data\n",
        "data_vectors = np.array([get_phrase_embedding(l) for l in data[:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "56b621dc",
      "metadata": {
        "id": "56b621dc"
      },
      "outputs": [],
      "source": [
        "class MyDb:\n",
        "    def __init__(self, sentences, sentence_embedings):\n",
        "        self._dim = len(sentence_embedings[0])\n",
        "        self._faiss = faiss.IndexFlatL2(self._dim )\n",
        "        self._faiss.add(sentence_embedings)\n",
        "        self._sentences = sentences\n",
        "\n",
        "    def get_similar(self, query_embeddings, k=5):\n",
        "        result = []\n",
        "        if (len(query_embeddings) == 0) or (len(query_embeddings[0])==0):\n",
        "            raise Exception(\"query_embeddings должно быть списком векторов-запросов для поиска похожих\")\n",
        "        d, i = self._faiss.search(x= query_embeddings, k = k)\n",
        "\n",
        "        for r in i:\n",
        "            r_sent = self._sentences[i]\n",
        "            result.append(r_sent)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "c8d0d0b9",
      "metadata": {
        "id": "c8d0d0b9"
      },
      "outputs": [],
      "source": [
        "MDB = MyDb(data, data_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "04636235",
      "metadata": {
        "id": "04636235",
        "outputId": "c0dc8964-10c0-400a-949a-bec89f70fd96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "type(data_vectors[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9hlHZAJAL1qL",
      "metadata": {
        "id": "9hlHZAJAL1qL"
      },
      "source": [
        "## 3.4. Применение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "2jbRh4UuL1qP",
      "metadata": {
        "id": "2jbRh4UuL1qP"
      },
      "outputs": [],
      "source": [
        "def find_nearest(query, k=10):\n",
        "    emb = get_phrase_embedding(query)\n",
        "    result = MDB.get_similar(np.array([emb]), k)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "524d0aa5",
      "metadata": {
        "id": "524d0aa5",
        "outputId": "8a7d1ae6-0f08-4317-f1dd-4275de33ac86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['Can you eat potatoes raw?\\n',\n",
              "         'What is the right way to boil potatos for potato salad?\\n',\n",
              "         'How do you can sliced potatoes?\\n',\n",
              "         'What is the best way to eat raw potatoes?\\n',\n",
              "         'How do you eat salad?\\n',\n",
              "         'Eating Healthy: Is eating mashed potatoes bad for me?\\n',\n",
              "         'What can be cooked with sweet potato butter?\\n',\n",
              "         'What are the advantages/disadvantages of eating potatoes?\\n',\n",
              "         'How do you make chicken soup with frozen chicken?\\n',\n",
              "         'Food: Is it safe to eat potatoes that have sprouted?\\n']],\n",
              "       dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "find_nearest(\"How to eat potatoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "b1e4393d",
      "metadata": {
        "id": "b1e4393d",
        "outputId": "2e0c92cd-f531-474d-cae0-8510ce756d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['Are all spiders venomous?\\n',\n",
              "         'How dangerous are corn snakes, and are they venomous?\\n',\n",
              "         'Why are some spiders venomous?\\n',\n",
              "         'Which snake has most dangerous venom?\\n',\n",
              "         'How did snakes evolve to have venom?\\n',\n",
              "         'Which venomous spiders are in California?\\n',\n",
              "         'What are the venomous spiders of Minnesota?\\n',\n",
              "         'What are the effects of a venomous snake bite?\\n',\n",
              "         \"What's the difference between venom and poison?\\n\",\n",
              "         'What is the difference between poison and venom?\\n']],\n",
              "       dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "find_nearest(\"Are all snakes venomous?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-UvAH08oTvf"
      },
      "id": "Z-UvAH08oTvf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}