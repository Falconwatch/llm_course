{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Falconwatch/llm_course/blob/main/HW1/%D0%94%D0%971.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oY3HD3eyIyC7",
      "metadata": {
        "id": "oY3HD3eyIyC7"
      },
      "source": [
        "# Описание ДЗ1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRly-JkgJOo_",
      "metadata": {
        "id": "rRly-JkgJOo_"
      },
      "source": [
        "На основе семинара 1 предложите 2 метода улучшения построения эмбеддингов вопросов на основе word vectors.\n",
        "\n",
        "За задание можно получить максимум 10 баллов.\n",
        "\n",
        "За каждый метод можно получить максимум 5 баллов.\n",
        "Разбалловка:\n",
        "*   Воспроизводимость и читабельность кода - 1 балл.\n",
        "*   Корректность метода - 1 балл.\n",
        "*   Описание метода в техническом отчете - 2 балла.\n",
        "*   Иновационность - 1 балл.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Информация о сабмите"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Щербаков Игорь Андреевич"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Технический отчет"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3",
      "metadata": {
        "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3"
      },
      "source": [
        "***Введите сюда** подробное описание предложенных методов и экспериментов, с помощью которых вы пришли именно к выбору этих методов. НЕ вставляйте код в эту часть. Описание должно состоять минимум из 2-4 абзацев и содержать следующее: тип модели, параметры, как вы выбрали параметры, какие дальнейшие модификации готовых решений и т.д. вы использовали. Сюда можно включить, например, некоторые хитрости вашей предварительной обработки, описание моделей и мотивацию их использования, описание деталей процесса обучения. Если нужно, вставьте сюда графики, математические формулы.*\n",
        "\n",
        "*Балл за «инновационность» будет присваиваться на основе содержания этой части. Если ваше отличие от бейзлайна это просто почистить тексты от стоп-слов или поменять одну базовую модель для построения word embeddings на другую, этот балл будет 0. Пробуйте разные подходы, модели, экспериментируйте с предварительной обработкой, параметрами и т. д. Можно переопределить уже существующий подход. Это нормально, что некоторые из ваших экспериментов не сработали так, как вы ожидали. Покажите нам, что вы проявили творческий подход и провели несколько экспериментов.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837fbc4d",
      "metadata": {
        "id": "837fbc4d"
      },
      "source": [
        "<h2>Возможные шаги</h2>\n",
        "Какие шаги были бы возможны:\n",
        "<ul> 1. Предобработка данных х</ul>\n",
        "<ul> 2. Подбор модели</ul>\n",
        "<ul> 3. Подбор параметров модели</ul>\n",
        "<ul> 4. Эксперименты с эмбедингами слов (добавление доп размерностей с новой информацией)</ul>\n",
        "<ul> 5. Изменеиение способа получения представления фразы на основе представлений слов</ul>\n",
        "<ul> 6. Модификация алгоритмов поиска похожих: повышение качества и скорости</ul>\n",
        "\n",
        "\n",
        "Подбор разных моделей и их параметров, кажется весьма обыденным, поэтому в этом ноутбуке рассмотрим следующщие подходы: предобработку данных, дополнительные размерности в эмбединг слова, представление фразы на основе слов.\n",
        "\n",
        "<h2>Что попробовал</h2>\n",
        "<h3>Предобработка</h3>\n",
        "В качестве дополнитльной предобработки были испоьзованы:\n",
        "<li> Стемминг алгоритмом Snowball - ок, полезно убирать лишние формы слов, часто опечатки в конце\n",
        "<li> Удаление стоп слов - отказался от него в пользу взвешивания\n",
        "<li> Формировать вектор предложения как взвешенную сумму векторов слов. В качестве весов - значения idf. Идейно подход заменяет выкидывание стоп-слов, занижая вес сильно распространённых слов\n",
        "\n",
        "<h3>Эксперименты с моделью</h3>\n",
        "<li>1\n",
        "<li>2\n",
        "\n",
        "\n",
        "\n",
        "<h3>Поиск похожих</h3>\n",
        "Алгоритм поиска похожих топиков на основе косинусного расстояния был заменнён на реализацию в FAISS, что позволило многократно ускорить поиск и повысить его релевантность.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. *Code*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1",
      "metadata": {
        "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1"
      },
      "source": [
        "*Введите сюда весь код, использованный для получения результатов. Добавьте несколько комментариев и подразделов для навигации по вашему решению.*\n",
        "\n",
        "*В этой части вам предстоит самостоятельно разработать решение задачи и предоставить воспроизводимый код:*\n",
        "- *Использование Python 3;*\n",
        "- *Содержит код для установки всех зависимостей;*\n",
        "- *Содержит код для загрузки всех используемых наборов данных*;\n",
        "- *Содержит код для воспроизведения ваших результатов (другими словами, если проверяющий загрузит ваш блокнот, он сможет выполнить код по ячейкам и получить результаты эксперимента, как описано в разделе методологии)*.\n",
        "\n",
        "\n",
        "*В результате ваш код будет оценен по следующим критериям:*\n",
        "- ***Читаемость**: ваш код должен быть хорошо структурирован, желательно с указанием частей вашего подхода (предварительная обработка, обучение модели, тестирование модели и т. д.).*\n",
        "- ***Воспроизводимость**: ваш код должен воспроизводиться без ошибок в режиме «Выполнить все» (получение экспериментальной части).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.1 Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82a7f821",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a7f821",
        "outputId": "34cc9af3-0a83-4ae4-d51b-4802b836cc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "#Установка бибилотек в окружение\n",
        "#!pip install -q -r requirements.txt\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9RQzpKJkLczr",
      "metadata": {
        "id": "9RQzpKJkLczr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import gensim.downloader as api\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.2 Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdce2db4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdce2db4",
        "outputId": "d769a8f9-0f1f-45f3-d1eb-8029be022e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-05 17:26:42--  https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [text/plain]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M   177MB/s    in 0.2s    \n",
            "\n",
            "2024-06-05 17:26:43 (177 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "\n",
        "#!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "#!wget https://yadi.sk/i/BPQrUu1NaTduEw -O ./quora.txt\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt -O ./quora.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9e25a5",
      "metadata": {
        "id": "6a9e25a5"
      },
      "outputs": [],
      "source": [
        "data = np.array(list(open(\"./quora.txt\", encoding=\"utf-8\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "oAORpF10drNu"
      },
      "id": "oAORpF10drNu"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "TOKENIZER = WordPunctTokenizer()\n",
        "STEMMER = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "LTfN7SCiduU8"
      },
      "id": "LTfN7SCiduU8",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def preprocess_phrase(phrase):\n",
        "  lower_phrase = phrase.lower()\n",
        "  lower_phrase = lower_phrase.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  tokens = TOKENIZER.tokenize(lower_phrase)\n",
        "  stemmed_tokens = [STEMMER.stem(t) for t in tokens]\n",
        "\n",
        "  return stemmed_tokens\n",
        "\n",
        "preprocess_phrase(\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA2H4OYdd1d8",
        "outputId": "22dd7d55-cd16-40c0-d3ea-c80e5fd948d2"
      },
      "id": "MA2H4OYdd1d8",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'there']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = [preprocess_phrase(d) for d in data[:]]"
      ],
      "metadata": {
        "id": "2WTfeLCheqxb"
      },
      "id": "2WTfeLCheqxb",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Формируем глобальный словарь частот\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "data_for_tfidf = [\" \".join(ws) for ws in preprocessed_data]\n",
        "vectorizer.fit_transform(data_for_tfidf)\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "idfs = vectorizer.idf_\n",
        "idfs_normalised = idfs/np.max(idfs)\n",
        "\n",
        "WORDS_IDFS = defaultdict(lambda: 1.0, {w:i for w,i in zip(words, idfs_normalised)})\n",
        "\n",
        "print(WORDS_IDFS[\"what\"])\n",
        "print(WORDS_IDFS[\"is\"])\n",
        "print(WORDS_IDFS[\"dog\"])\n",
        "print(WORDS_IDFS[\"?\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV_HVOVMfDtn",
        "outputId": "bdf346c9-fd62-4e3f-ff71-0d4366ad9c2b"
      },
      "id": "uV_HVOVMfDtn",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14412748635454461\n",
            "0.1572065412341149\n",
            "0.48880240698138905\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_phrase_and_get_idf(phrase):\n",
        "  preprocessed_phrase = preprocess_phrase(phrase)\n",
        "  phrase_idfs = [WORDS_IDFS[word] for word in preprocessed_phrase]\n",
        "  phrase_idfs = phrase_idfs/np.sum(phrase_idfs)\n",
        "  return preprocessed_phrase, np.array(phrase_idfs)\n",
        "\n",
        "process_phrase_and_get_idf(\"what is dog?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfqXH32JiH65",
        "outputId": "371b7cbe-54d4-4027-a73d-9f6fc2a4a36c"
      },
      "id": "KfqXH32JiH65",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['what', 'is', 'dog'], array([0.18240835, 0.19896126, 0.61863039]))"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.3. Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "dec65766",
      "metadata": {
        "id": "dec65766"
      },
      "outputs": [],
      "source": [
        "GLOVE_MODEL = api.load('glove-twitter-100')\n",
        "GLOVE_WORDS_IN_DICT = MODEL.key_to_index.keys()\n",
        "\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "FT_MODEL = FastText(vector_size=128, window=3, min_count=1)  # instantiate\n",
        "FT_MODEL.build_vocab(corpus_iterable=preprocessed_data)\n",
        "FT_MODEL.train(corpus_iterable=preprocessed_data, total_examples=len(preprocessed_data), epochs=3)\n",
        "FT_WORDS_IN_DICT = model.wv.key_to_index.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "-ySx_n28KGAX",
      "metadata": {
        "id": "-ySx_n28KGAX"
      },
      "outputs": [],
      "source": [
        "def get_phrase_embedding(phrase,\n",
        "                         models:list,\n",
        "                         words_dicts:list):\n",
        "  full_vector_size = 0\n",
        "  for model in models:\n",
        "    full_vector_size += model.vector_size\n",
        "\n",
        "  empty_vector = np.zeros([full_vector_size], dtype='float32')\n",
        "  word_vectors = []\n",
        "\n",
        "  phrase_words, words_weights = process_phrase_and_get_idf(phrase)\n",
        "\n",
        "  #перебираю слова в фразе\n",
        "  for word in phrase_words:\n",
        "    #сюда буду класть эмбединги по слову\n",
        "    one_word_vectors = list()\n",
        "    #перебираю доступные модели\n",
        "    for i in range(len(models)):\n",
        "      model = models[i]\n",
        "      words_in_dict = words_dicts[i]\n",
        "\n",
        "      vector_store = model.wv if type(model) is FastText else model\n",
        "\n",
        "      if type(model) is FastText:\n",
        "        vector_store = model.wv\n",
        "      else:\n",
        "        vector_store = model\n",
        "\n",
        "      if word in words_in_dict:\n",
        "        one_word_vectors.append(vector_store.get_vector(word))\n",
        "      else:\n",
        "        one_word_vectors.append(np.zeros([model.vector_size], dtype='float32'))\n",
        "    one_word_vector = np.concatenate(one_word_vectors)\n",
        "    word_vectors.append(one_word_vector)\n",
        "\n",
        "  phrase_vector = np.dot(words_weights, word_vectors)\n",
        "  return phrase_vector\n",
        "\n",
        "my_models = [GLOVE_MODEL, FT_MODEL]\n",
        "my_words_dicts = [GLOVE_WORDS_IN_DICT, FT_WORDS_IN_DICT]\n",
        "\n",
        "t1 = get_phrase_embedding(\"What is dog?\", my_models, my_words_dicts)\n",
        "#t2 = get_phrase_embedding(\"What is dog?\", GLOVE_MODEL, GLOVE_WORDS_IN_DICT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "FEpFpJsfUdA4",
      "metadata": {
        "id": "FEpFpJsfUdA4"
      },
      "outputs": [],
      "source": [
        "# compute vector embedding for all lines in data\n",
        "data_vectors = np.array([ get_phrase_embedding(l, my_models, my_words_dicts) for l in data[:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "56b621dc",
      "metadata": {
        "id": "56b621dc"
      },
      "outputs": [],
      "source": [
        "class MyDb:\n",
        "    def __init__(self, sentences, sentence_embedings):\n",
        "        self._dim = len(sentence_embedings[0])\n",
        "        self._faiss = faiss.IndexFlatL2(self._dim )\n",
        "        self._faiss.add(sentence_embedings)\n",
        "        self._sentences = sentences\n",
        "\n",
        "    def get_similar(self, query_embeddings, k=5):\n",
        "        result = []\n",
        "        if (len(query_embeddings) == 0) or (len(query_embeddings[0])==0):\n",
        "            raise Exception(\"query_embeddings должно быть списком векторов-запросов для поиска похожих\")\n",
        "        d, i = self._faiss.search(x= query_embeddings, k = k)\n",
        "\n",
        "        for r in i:\n",
        "            r_sent = self._sentences[i]\n",
        "            result.append(r_sent)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "c8d0d0b9",
      "metadata": {
        "id": "c8d0d0b9"
      },
      "outputs": [],
      "source": [
        "MDB = MyDb(data, data_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9hlHZAJAL1qL",
      "metadata": {
        "id": "9hlHZAJAL1qL"
      },
      "source": [
        "## 3.4. Применение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "2jbRh4UuL1qP",
      "metadata": {
        "id": "2jbRh4UuL1qP"
      },
      "outputs": [],
      "source": [
        "def find_nearest(query, my_models, my_words_dicts, k=10):\n",
        "    emb = get_phrase_embedding(query, my_models, my_words_dicts)\n",
        "    result = MDB.get_similar(np.array([emb]), k)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "524d0aa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524d0aa5",
        "outputId": "94281e0d-8e77-46a1-c7b6-6751d68218c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['How can tortoises eat tomatoes?\\n',\n",
              "         'Can you eat potatoes raw?\\n',\n",
              "         'How dangerous is it to eat raw eggs?\\n',\n",
              "         'How safe is it to eat raw hotdogs?\\n',\n",
              "         'How do you eat raw egg and salad in Japan?\\n',\n",
              "         'How safe is it to eat raw eggs?\\n',\n",
              "         'Can you eat fish and eggs together?\\n',\n",
              "         'How safe is it to eat raw steak?\\n',\n",
              "         'What can eating raw meat lead to?\\n',\n",
              "         'How do you eat coconut crabs?\\n']], dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "find_nearest(\"How to eat potatoes\", my_models, my_words_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "b1e4393d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e4393d",
        "outputId": "4df5869c-e8b3-480c-8d24-e66104b01998"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['Are all spiders venomous?\\n',\n",
              "         'How dangerous are corn snakes, and are they venomous?\\n',\n",
              "         'Are all insects omnivorous/herbivorous?\\n',\n",
              "         'Are all types of flesh edible?\\n',\n",
              "         \"What are all of Henry Rollins's tattoos?\\n\",\n",
              "         'Are there mutant snakes?\\n', 'Do all cobras spit venom?\\n',\n",
              "         'Why are some spiders venomous?\\n',\n",
              "         'Are all-white pit bulls rare?\\n',\n",
              "         'What are the venomous spiders of Minnesota?\\n']], dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "find_nearest(\"Are all snakes venomous?\", my_models, my_words_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-UvAH08oTvf"
      },
      "id": "Z-UvAH08oTvf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "wcrwjOXouplX"
      },
      "id": "wcrwjOXouplX"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziHdYA05uztN",
        "outputId": "9809f71c-d4f3-4111-d5e4-eb7b34ae8a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ziHdYA05uztN",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43868568, 61294640)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.vector_size"
      ],
      "metadata": {
        "id": "fSrVMjDqu6zb",
        "outputId": "b0ba8f53-7770-446b-a8f9-5e766ddcdd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fSrVMjDqu6zb",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model) is FastText"
      ],
      "metadata": {
        "id": "iuGHgCY00qKg",
        "outputId": "579f7622-6179-40fa-b358-a33a42c2131d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iuGHgCY00qKg",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.get_vector(\"hello\")"
      ],
      "metadata": {
        "id": "Yfha1kqw0gxt",
        "outputId": "6b9f9873-232c-44a3-bab8-020acaee4f53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Yfha1kqw0gxt",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.6862627 ,  1.1529286 , -0.03999156,  0.41668805, -0.08775536,\n",
              "        0.4633573 , -0.93307227,  0.03430814,  1.1212194 ,  0.15501061,\n",
              "        0.99429667, -1.0509496 , -0.52912676, -0.16949584, -0.27724844,\n",
              "        1.0890815 , -0.6406821 ,  0.3036683 , -0.23967867,  1.5083961 ,\n",
              "       -1.0448737 ,  1.1400263 ,  1.0975782 ,  0.5451354 ,  0.7920713 ,\n",
              "        0.30796623, -0.7411187 ,  0.2733937 , -0.362613  ,  0.304511  ,\n",
              "        0.71818846, -1.116722  ,  0.07348555,  0.9912328 ,  0.05032878,\n",
              "       -0.56667906,  0.7744326 , -0.07837544, -0.6709083 ,  0.08309229,\n",
              "       -0.5199062 , -0.15987472, -0.9971523 , -0.78810054,  0.21179396,\n",
              "        0.40839264,  0.74007696, -0.12652978,  0.09335849,  0.41339588,\n",
              "       -0.02734203, -1.0381933 ,  0.44872716,  0.14822537,  1.1556053 ,\n",
              "        0.3851937 , -0.12182264,  0.7659448 , -0.9767002 , -0.6001767 ,\n",
              "        0.826352  ,  1.0730821 , -0.80103076,  0.4354746 ,  0.44758016,\n",
              "       -0.4289955 , -0.85899514,  0.9181729 ,  0.9911824 ,  1.3928248 ,\n",
              "       -1.1006314 ,  0.49548024,  0.89525115,  0.03743237, -0.48812896,\n",
              "        0.3558207 ,  0.12059788,  0.45345923, -0.69789636, -1.392117  ,\n",
              "        0.31916717, -0.50440913, -0.1259678 ,  0.01534516, -1.3669355 ,\n",
              "        1.4661738 , -1.379104  ,  0.5194515 , -0.45477727,  1.6378567 ,\n",
              "        1.1459526 ,  1.2310089 , -0.398338  ,  0.3378456 ,  0.4548556 ,\n",
              "        0.5762566 ,  0.2070211 , -0.28194258, -0.8562223 , -0.5777426 ,\n",
              "       -0.3394297 , -0.81509525, -0.5420757 , -0.6700699 ,  0.14220966,\n",
              "        1.1175596 ,  0.5584743 ,  0.10519134,  0.05513847,  0.083392  ,\n",
              "        0.9889408 ,  0.1875237 , -0.02814301,  0.07401911,  0.42756182,\n",
              "        0.10613494,  0.81785226, -0.7369138 ,  0.5410343 , -0.15416549,\n",
              "        1.0240566 , -0.68762577, -0.334507  ,  0.3474284 , -0.31233814,\n",
              "       -0.6064879 , -1.5213811 ,  0.15409502], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.word_vec(\"hello\")"
      ],
      "metadata": {
        "id": "zcboZ_WKxyGm",
        "outputId": "55bb6083-00c4-46e8-fd42-250a7975028f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zcboZ_WKxyGm",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-3c2124849c76>:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  model.wv.word_vec(\"hello\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.6862627 ,  1.1529286 , -0.03999156,  0.41668805, -0.08775536,\n",
              "        0.4633573 , -0.93307227,  0.03430814,  1.1212194 ,  0.15501061,\n",
              "        0.99429667, -1.0509496 , -0.52912676, -0.16949584, -0.27724844,\n",
              "        1.0890815 , -0.6406821 ,  0.3036683 , -0.23967867,  1.5083961 ,\n",
              "       -1.0448737 ,  1.1400263 ,  1.0975782 ,  0.5451354 ,  0.7920713 ,\n",
              "        0.30796623, -0.7411187 ,  0.2733937 , -0.362613  ,  0.304511  ,\n",
              "        0.71818846, -1.116722  ,  0.07348555,  0.9912328 ,  0.05032878,\n",
              "       -0.56667906,  0.7744326 , -0.07837544, -0.6709083 ,  0.08309229,\n",
              "       -0.5199062 , -0.15987472, -0.9971523 , -0.78810054,  0.21179396,\n",
              "        0.40839264,  0.74007696, -0.12652978,  0.09335849,  0.41339588,\n",
              "       -0.02734203, -1.0381933 ,  0.44872716,  0.14822537,  1.1556053 ,\n",
              "        0.3851937 , -0.12182264,  0.7659448 , -0.9767002 , -0.6001767 ,\n",
              "        0.826352  ,  1.0730821 , -0.80103076,  0.4354746 ,  0.44758016,\n",
              "       -0.4289955 , -0.85899514,  0.9181729 ,  0.9911824 ,  1.3928248 ,\n",
              "       -1.1006314 ,  0.49548024,  0.89525115,  0.03743237, -0.48812896,\n",
              "        0.3558207 ,  0.12059788,  0.45345923, -0.69789636, -1.392117  ,\n",
              "        0.31916717, -0.50440913, -0.1259678 ,  0.01534516, -1.3669355 ,\n",
              "        1.4661738 , -1.379104  ,  0.5194515 , -0.45477727,  1.6378567 ,\n",
              "        1.1459526 ,  1.2310089 , -0.398338  ,  0.3378456 ,  0.4548556 ,\n",
              "        0.5762566 ,  0.2070211 , -0.28194258, -0.8562223 , -0.5777426 ,\n",
              "       -0.3394297 , -0.81509525, -0.5420757 , -0.6700699 ,  0.14220966,\n",
              "        1.1175596 ,  0.5584743 ,  0.10519134,  0.05513847,  0.083392  ,\n",
              "        0.9889408 ,  0.1875237 , -0.02814301,  0.07401911,  0.42756182,\n",
              "        0.10613494,  0.81785226, -0.7369138 ,  0.5410343 , -0.15416549,\n",
              "        1.0240566 , -0.68762577, -0.334507  ,  0.3474284 , -0.31233814,\n",
              "       -0.6064879 , -1.5213811 ,  0.15409502], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jEka9ZUxTVg"
      },
      "id": "4jEka9ZUxTVg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}