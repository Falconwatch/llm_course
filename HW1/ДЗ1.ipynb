{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Falconwatch/llm_course/blob/main/HW1/%D0%94%D0%971.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oY3HD3eyIyC7",
      "metadata": {
        "id": "oY3HD3eyIyC7"
      },
      "source": [
        "# Описание ДЗ1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRly-JkgJOo_",
      "metadata": {
        "id": "rRly-JkgJOo_"
      },
      "source": [
        "На основе семинара 1 предложите 2 метода улучшения построения эмбеддингов вопросов на основе word vectors.\n",
        "\n",
        "За задание можно получить максимум 10 баллов.\n",
        "\n",
        "За каждый метод можно получить максимум 5 баллов.\n",
        "Разбалловка:\n",
        "*   Воспроизводимость и читабельность кода - 1 балл.\n",
        "*   Корректность метода - 1 балл.\n",
        "*   Описание метода в техническом отчете - 2 балла.\n",
        "*   Иновационность - 1 балл.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Информация о сабмите"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Щербаков Игорь Андреевич"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Технический отчет"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837fbc4d",
      "metadata": {
        "id": "837fbc4d"
      },
      "source": [
        "<h2> Что было сделано</h2>\n",
        "Раздел содержит испробованные методы, как попавшие в итоговое решение, так и отвергнутые после проверки\n",
        "\n",
        "<h3> Предобработка</h3>\n",
        "<li> Знаки пунктуации выкинул\n",
        "<li> Стемминг алгоритмом Snowball - ок, полезно убирать лишние формы слов, часто опечатки в конце\n",
        "<li> Удаление стоп слов - отказался от него в пользу взвешивания по idf (см ниже раздел Получение эмбедингов)\n",
        "\n",
        "<h3>Модели</h3>\n",
        "<li> Пробовал другие модели glove, обученне на бОльших датасетах в итоге остановился на той, что была использована на семинаре из-за веса самой модели\n",
        "<li> Обучил на нашем сете FastText, пробовал разные значения размера эмбединга, окна и частотности вхождения. В итоговом решении оставил небольшое количестов эпох и +- оптимальные параметры - чтобы сократить время исполнения ноутбука. Длину вектора эмбединга выбрал равной 100, чтобы лучше стакать веткор с вектором glove (см раздел Получение эмбедингов), и не перетягивать на fasttext внимание при расчёте похожести\n",
        "\n",
        "<h3>Получение эмбедингов</h3>\n",
        "<li> Решил эмбеднг слова делать путём конкатенации эмбедингов из разных моделей. В итоговом решении стклеиваются эмбединги Glove и FastText.\n",
        "<li> Для успешного стекинга вектора-эмбединги нормирую в пределах модели, так чтобы сумма элемнетов веткоров была равно 1\n",
        "<li> От простой суммы или среднего для получения эмбединга предложения отказался в пользу взвешенной суммы слов. В качестве веса - значение idf слова, вычисленное на всём датасете. При расчёте весов слов в предложении idf нормируются, чтобы в сумме давать единицу. После чего эмбединг каждого слова умножается на соответсвующий вес\n",
        "\n",
        "\n",
        "<h3>Поиск похожих</h3>\n",
        "Алгоритм поиска похожих топиков на основе косинусного расстояния был заменнён на реализацию в FAISS, что позволило многократно ускорить поиск и повысить его релевантность.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. *Code*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.1 Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82a7f821",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a7f821",
        "outputId": "ef31e6a1-1dde-42ea-90e4-a94092f2f4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "#Установка бибилотек в окружение\n",
        "#!pip install -q -r requirements.txt\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9RQzpKJkLczr",
      "metadata": {
        "id": "9RQzpKJkLczr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import gensim.downloader as api\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import faiss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.2 Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cdce2db4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdce2db4",
        "outputId": "7259ddd4-9c94-4093-c2ed-0c6b7f7aee8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-06 09:14:36--  https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33813903 (32M) [text/plain]\n",
            "Saving to: ‘./quora.txt’\n",
            "\n",
            "./quora.txt         100%[===================>]  32.25M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-06 09:14:37 (247 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the data:\n",
        "\n",
        "#!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "#!wget https://yadi.sk/i/BPQrUu1NaTduEw -O ./quora.txt\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Falconwatch/llm_course/main/HW1/quora.txt -O ./quora.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a9e25a5",
      "metadata": {
        "id": "6a9e25a5"
      },
      "outputs": [],
      "source": [
        "data = np.array(list(open(\"./quora.txt\", encoding=\"utf-8\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "oAORpF10drNu"
      },
      "id": "oAORpF10drNu"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "TOKENIZER = WordPunctTokenizer()\n",
        "STEMMER = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "LTfN7SCiduU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebd2cd1-eba7-43e8-8631-843e364e3b28"
      },
      "id": "LTfN7SCiduU8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def preprocess_phrase(phrase):\n",
        "  lower_phrase = phrase.lower()\n",
        "  lower_phrase = lower_phrase.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  tokens = TOKENIZER.tokenize(lower_phrase)\n",
        "  stemmed_tokens = [STEMMER.stem(t) for t in tokens]\n",
        "\n",
        "  return stemmed_tokens\n",
        "\n",
        "preprocess_phrase(\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA2H4OYdd1d8",
        "outputId": "e061af93-4c51-4e92-e6f3-8ff5675edb5d"
      },
      "id": "MA2H4OYdd1d8",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'there']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = [preprocess_phrase(d) for d in data[:]]"
      ],
      "metadata": {
        "id": "2WTfeLCheqxb"
      },
      "id": "2WTfeLCheqxb",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Формируем глобальный словарь частот\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "data_for_tfidf = [\" \".join(ws) for ws in preprocessed_data]\n",
        "vectorizer.fit_transform(data_for_tfidf)\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "idfs = vectorizer.idf_\n",
        "idfs_normalised = idfs/np.max(idfs)\n",
        "\n",
        "WORDS_IDFS = defaultdict(lambda: 1.0, {w:i for w,i in zip(words, idfs_normalised)})\n",
        "\n",
        "print(WORDS_IDFS[\"what\"])\n",
        "print(WORDS_IDFS[\"is\"])\n",
        "print(WORDS_IDFS[\"dog\"])\n",
        "print(WORDS_IDFS[\"?\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV_HVOVMfDtn",
        "outputId": "f9f9ba1b-d1a6-4591-c826-2fd249bd3d65"
      },
      "id": "uV_HVOVMfDtn",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14412748635454461\n",
            "0.1572065412341149\n",
            "0.48880240698138905\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_phrase_and_get_idf(phrase):\n",
        "  preprocessed_phrase = preprocess_phrase(phrase)\n",
        "  phrase_idfs = [WORDS_IDFS[word] for word in preprocessed_phrase]\n",
        "  phrase_idfs = phrase_idfs/np.sum(phrase_idfs)\n",
        "  return preprocessed_phrase, np.array(phrase_idfs)\n",
        "\n",
        "process_phrase_and_get_idf(\"what is dog?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfqXH32JiH65",
        "outputId": "24f8e93a-8cab-4618-de64-c559551222fa"
      },
      "id": "KfqXH32JiH65",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['what', 'is', 'dog'], array([0.18240835, 0.19896126, 0.61863039]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.3. Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dec65766",
      "metadata": {
        "id": "dec65766"
      },
      "outputs": [],
      "source": [
        "GLOVE_MODEL = api.load('glove-twitter-100')\n",
        "GLOVE_WORDS_IN_DICT = GLOVE_MODEL.key_to_index.keys()\n",
        "\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "FT_MODEL = FastText(vector_size=100, window=3, min_count=1)  # instantiate\n",
        "FT_MODEL.build_vocab(corpus_iterable=preprocessed_data)\n",
        "FT_MODEL.train(corpus_iterable=preprocessed_data, total_examples=len(preprocessed_data), epochs=3)\n",
        "FT_WORDS_IN_DICT = FT_MODEL.wv.key_to_index.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "-ySx_n28KGAX",
      "metadata": {
        "id": "-ySx_n28KGAX"
      },
      "outputs": [],
      "source": [
        "def get_phrase_embedding(phrase,\n",
        "                         models:list,\n",
        "                         words_dicts:list):\n",
        "  full_vector_size = 0\n",
        "  for model in models:\n",
        "    full_vector_size += model.vector_size\n",
        "\n",
        "  empty_vector = np.zeros([full_vector_size], dtype='float32')\n",
        "  word_vectors = []\n",
        "\n",
        "  phrase_words, words_weights = process_phrase_and_get_idf(phrase)\n",
        "\n",
        "  #перебираю слова в фразе\n",
        "  for word in phrase_words:\n",
        "    #сюда буду класть эмбединги по слову\n",
        "    one_word_vectors = list()\n",
        "    #перебираю доступные модели\n",
        "    for i in range(len(models)):\n",
        "      model = models[i]\n",
        "      words_in_dict = words_dicts[i]\n",
        "\n",
        "      vector_store = model.wv if type(model) is FastText else model\n",
        "\n",
        "      if type(model) is FastText:\n",
        "        vector_store = model.wv\n",
        "      else:\n",
        "        vector_store = model\n",
        "\n",
        "      if word in words_in_dict:\n",
        "        one_word_one_model_vector = vector_store.get_vector(word)\n",
        "        one_word_one_model_vector = one_word_one_model_vector/np.sum(one_word_one_model_vector)\n",
        "        one_word_vectors.append(one_word_one_model_vector)\n",
        "      else:\n",
        "        one_word_vectors.append(np.zeros([model.vector_size], dtype='float32'))\n",
        "    one_word_vector = np.concatenate(one_word_vectors)\n",
        "    word_vectors.append(one_word_vector)\n",
        "\n",
        "  phrase_vector = np.dot(words_weights, word_vectors)\n",
        "  return phrase_vector\n",
        "\n",
        "my_models = [GLOVE_MODEL, FT_MODEL]\n",
        "my_words_dicts = [GLOVE_WORDS_IN_DICT, FT_WORDS_IN_DICT]\n",
        "\n",
        "t1 = get_phrase_embedding(\"What is dog?\", my_models, my_words_dicts)\n",
        "#t2 = get_phrase_embedding(\"What is dog?\", GLOVE_MODEL, GLOVE_WORDS_IN_DICT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "FEpFpJsfUdA4",
      "metadata": {
        "id": "FEpFpJsfUdA4"
      },
      "outputs": [],
      "source": [
        "# compute vector embedding for all lines in data\n",
        "data_vectors = np.array([ get_phrase_embedding(l, my_models, my_words_dicts) for l in data[:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "56b621dc",
      "metadata": {
        "id": "56b621dc"
      },
      "outputs": [],
      "source": [
        "class MyDb:\n",
        "    def __init__(self, sentences, sentence_embedings):\n",
        "        self._dim = len(sentence_embedings[0])\n",
        "        self._faiss = faiss.IndexFlatL2(self._dim )\n",
        "        self._faiss.add(sentence_embedings)\n",
        "        self._sentences = sentences\n",
        "\n",
        "    def get_similar(self, query_embeddings, k=5):\n",
        "        result = []\n",
        "        if (len(query_embeddings) == 0) or (len(query_embeddings[0])==0):\n",
        "            raise Exception(\"query_embeddings должно быть списком векторов-запросов для поиска похожих\")\n",
        "        d, i = self._faiss.search(x= query_embeddings, k = k)\n",
        "\n",
        "        for r in i:\n",
        "            r_sent = self._sentences[i]\n",
        "            result.append(r_sent)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c8d0d0b9",
      "metadata": {
        "id": "c8d0d0b9"
      },
      "outputs": [],
      "source": [
        "MDB = MyDb(data, data_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9hlHZAJAL1qL",
      "metadata": {
        "id": "9hlHZAJAL1qL"
      },
      "source": [
        "## 3.4. Применение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2jbRh4UuL1qP",
      "metadata": {
        "id": "2jbRh4UuL1qP"
      },
      "outputs": [],
      "source": [
        "def find_nearest(query, my_models, my_words_dicts, k=10):\n",
        "    emb = get_phrase_embedding(query, my_models, my_words_dicts)\n",
        "    result = MDB.get_similar(np.array([emb]), k)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "524d0aa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524d0aa5",
        "outputId": "4ed5bbfa-941b-42d9-b1ed-c74cea0ca775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['Can you eat potatoes raw?\\n', 'Are beans potatoes?\\n',\n",
              "         'Why are potatoes green?\\n',\n",
              "         'What are the advantages/disadvantages of eating potatoes?\\n',\n",
              "         'How do red potatoes differ from white potatoes?\\n',\n",
              "         'Can guinea pigs eat potatoes?\\n', 'Why is my potato green?\\n',\n",
              "         'What is the best way to eat raw potatoes?\\n',\n",
              "         'What is the right way to boil potatos for potato salad?\\n',\n",
              "         'Eating Healthy: Is eating mashed potatoes bad for me?\\n']],\n",
              "       dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "find_nearest(\"How to eat potatoes\", my_models, my_words_dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b1e4393d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e4393d",
        "outputId": "81e61010-5330-44bf-a560-9e19c501ebf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([['To be or not to be?\\n', 'How to talk to guys?\\n',\n",
              "         'Who to go to iit?\\n', 'What does the \"to it\" refer to?\\n',\n",
              "         'Things to do to kill time?\\n', 'To be deleted?\\n',\n",
              "         '? to be deleted\\n', '? (To be deleted)\\n',\n",
              "         \"What's the best album to work to?\\n\",\n",
              "         'What is the best way to choose colleges to apply to?\\n']],\n",
              "       dtype='<U1170')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "find_nearest(\"To be or not to be?\", my_models, my_words_dicts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}